{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Scaling"
      ],
      "metadata": {
        "id": "wqfYnpHy6TOw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OP81v7bAyVQl"
      },
      "outputs": [],
      "source": [
        "# Some of this code is from 1nhee/space on github\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a DataFrame with some data.\n",
        "data_train = pd.DataFrame({\n",
        "    'age': [20, 25, 30, 35, 40],\n",
        "    'income': [50000, 60000, 70000, 80000, 90000],\n",
        "    'height' : [1.6, 1.7, 1.8, 1.5, 1.55]\n",
        "})\n",
        "data_test = pd.DataFrame({\n",
        "    'age': [18, 5, 20, 30, 17],\n",
        "    'income': [9000,8000,7000,6000,5000],\n",
        "    'height' : [1.2, 2.0, 1.9, 1.5, 1.6]\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "id": "DIrVZdCN4ZMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test"
      ],
      "metadata": {
        "id": "2-bHeE6J4a-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Scale the data using StandardScaler.\n",
        "scaler1 = StandardScaler()\n",
        "scaler1.fit(data_train[['age', 'income']])\n",
        "data_train[['age_std_scale', 'income_std_scale']] = scaler1.transform(data_train[['age', 'income']])\n",
        "data_test[['age_std_scale', 'income_std_scale']] = scaler1.transform(data_test[['age', 'income']])\n",
        "\n",
        "# Scale the data using MinMaxScaler.\n",
        "scaler2 = MinMaxScaler()\n",
        "scaler2.fit(data_train[['age', 'income']])\n",
        "data_train[['age_minmax_scale', 'income_minmax_scale']] = scaler2.transform(data_train[['age', 'income']])\n",
        "data_test[['age_minmax_scale', 'income_minmax_scale']] = scaler2.transform(data_test[['age', 'income']])\n"
      ],
      "metadata": {
        "id": "-XQEgvBf4VOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "id": "4QTo0Dro4O-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test"
      ],
      "metadata": {
        "id": "RshTvYWK4gDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can peak into the mean, the stdev of the std scaler\n",
        "# (from the data before the transformation)\n",
        "print (scaler1.mean_)\n",
        "print (scaler1.scale_)"
      ],
      "metadata": {
        "id": "I_zUSv7t2aeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can peak into the min, the max of the minmax scaler\n",
        "# (from the training data before the transformation)\n",
        "print (scaler2.data_min_)\n",
        "print (scaler2.data_max_)"
      ],
      "metadata": {
        "id": "u2mc4XTc2kOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data scaling is really easy, we could have done this \"manually\"\n",
        "min_ = data_train[[\"age\", \"income\"]].min()\n",
        "max_ = data_train[[\"age\", \"income\"]].max()\n",
        "print (min_)\n",
        "print (max_)"
      ],
      "metadata": {
        "id": "1-4wh7XT3K9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train[[\"age_minmax_scale2\", \"income_minmax_scale2\"]] = (data_train[[\"age\", \"income\"]]  - min_)/(max_ - min_)\n",
        "data_train"
      ],
      "metadata": {
        "id": "xk5iZ12Y4pRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test[[\"age_minmax_scale2\", \"income_minmax_scale2\"]] = (data_test[[\"age\", \"income\"]]  - min_)/(max_ - min_)\n",
        "data_test"
      ],
      "metadata": {
        "id": "YppVBBpJ3xBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outlier detection"
      ],
      "metadata": {
        "id": "IHRriGsL6YsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = pd.DataFrame({'a': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16],\n",
        "                   'b': [8,6,4,2, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32]})\n",
        "\n",
        "data\n"
      ],
      "metadata": {
        "id": "zY8Jf_dW6DQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outlier detection using z-score"
      ],
      "metadata": {
        "id": "CJYc38XzNXt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# z-score done using standard scaler to get z-scores\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(data[['a', 'b']])\n",
        "data[['a_scaled', 'b_scaled']] = scaler.transform(data[['a', 'b']])\n",
        "\n",
        "print (\"Data after z-score computation (standard scaler\")\n",
        "data"
      ],
      "metadata": {
        "id": "zgGqJggkNZQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mark all those with zscore >1.6 or <-1.6 as outliers (missing data)\n",
        "# Always safer to create a new column rather than working \"in place\"\n",
        "data[\"a_outliers_removed\"] = data[\"a\"].where(data[\"a_scaled\"].abs()<1.6)\n",
        "data[\"b_outliers_removed\"] = data[\"b\"].where(data[\"b_scaled\"].abs()<1.6)\n",
        "data"
      ],
      "metadata": {
        "id": "rEh56AzuGeo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's remove the rows with missing data\n",
        "data_rows_removed = data.dropna(axis=0) # dropping rows (you can also remove columns with axis=1)\n",
        "data_rows_removed"
      ],
      "metadata": {
        "id": "VTTRAoUqMAxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outlier detection using IQR"
      ],
      "metadata": {
        "id": "xYe1QQszNfUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create some fresh data first\n",
        "data = pd.DataFrame({'a': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 500, 12, 13, 14, 15, 16],\n",
        "                   'b': [8,6,4,2, 10, -100, 14, 16, 18, 20, 22, 24, 26, 28, 30, 100]})\n",
        "\n",
        "# Make sure you understand what is printed\n",
        "quantiles = data.quantile([0.25, 0.75])\n",
        "quantiles"
      ],
      "metadata": {
        "id": "osLCZytgNujM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IQR = quantiles.loc[0.75] - quantiles.loc[0.25]\n",
        "IQR"
      ],
      "metadata": {
        "id": "ZXqvVO1uOsKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's do IQR outlier removal with alpha = 1.5\n",
        "# This time let's do it in-place\n",
        "data[\"a\"] = data[\"a\"].where(\n",
        "    (data[\"a\"] <= quantiles[\"a\"][0.75] + IQR[\"a\"] * 1.5) &\n",
        "    (data[\"a\"] >= quantiles[\"a\"][0.25] - IQR[\"a\"] * 1.5))\n",
        "data[\"b\"] = data[\"b\"].where(\n",
        "    (data[\"b\"] <= quantiles[\"b\"][0.75] + IQR[\"b\"] * 1.5) &\n",
        "    (data[\"b\"] >= quantiles[\"b\"][0.25] - IQR[\"b\"] * 1.5))"
      ],
      "metadata": {
        "id": "Kt1rPmsyNycC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "jhMQpnpfOQaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_rows_removed = data.dropna(axis=0)\n",
        "data_rows_removed"
      ],
      "metadata": {
        "id": "V-OIovsUPRXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Missing Value Handling"
      ],
      "metadata": {
        "id": "_OVkyl1xQHQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create some fresh data first\n",
        "data = pd.DataFrame({\n",
        "    'a': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None, 12, 13, 14, 15, 16],\n",
        "    'b': [8,6,4,2, 10, None, 14, 16, 18, 20, 22, 24, 26, 28, 30, None],\n",
        "    'c' : [100, None, None, None, 5, None, None, None,100, None, None, None, 5, None, None, None]})\n",
        "\n"
      ],
      "metadata": {
        "id": "fIRrigY_P2gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that you can compute stats on columns with missing data\n",
        "# The missing data will be simply ignored\n",
        "print(data['a'].mean())\n",
        "print(data['b'].std())"
      ],
      "metadata": {
        "id": "LKVOju5vQUkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing columns with rate of missing values threshold\n"
      ],
      "metadata": {
        "id": "6irPRo3CQzRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many missing values in each column?\n",
        "num_missing = data.isna().sum()\n",
        "num_missing"
      ],
      "metadata": {
        "id": "qbni2Fe6QWVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_cols_removed = data.loc[:, num_missing<=6]\n",
        "data_cols_removed"
      ],
      "metadata": {
        "id": "gK9dVSaNQaEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imputation with mean and median"
      ],
      "metadata": {
        "id": "L8GOwpfhSIq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_cols_removed_mean_imp = data_cols_removed.fillna(data_cols_removed.mean())\n",
        "data_cols_removed_mean_imp"
      ],
      "metadata": {
        "id": "9-XIL2goRfc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_cols_removed_median_imp = data_cols_removed.fillna(data_cols_removed.median())\n",
        "data_cols_removed_median_imp"
      ],
      "metadata": {
        "id": "MGAiJKKkSUNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Transformations"
      ],
      "metadata": {
        "id": "7ldQnk1DSbcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create some fresh data first\n",
        "data = pd.DataFrame({\n",
        "    'a': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 12, 13, 14, 15, 16],\n",
        "    'b': [8,6,4,2, 10, 19, 14, 16, 18, 20, 22, 24, 26, 28, 30, 21]})"
      ],
      "metadata": {
        "id": "3XfQXkAmSWP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logarithm, Box-Cox transformation with lambda-2\n",
        "data[\"log_a\"] = np.log(data[\"a\"])\n",
        "data[\"box_cox2_b\"] = (np.power(data[\"b\"], 2) - 1) / 2\n",
        "data\n",
        "# could also use box-cox from here:\n",
        "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html\n"
      ],
      "metadata": {
        "id": "DEvK85B0S5Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bucketization/discretization\n",
        "data[\"a_bucketized\"] = pd.cut(data[\"a\"], bins=5) # number of bins, equally spaced\n",
        "data"
      ],
      "metadata": {
        "id": "mKFynoPRTG2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bucketization/discretization\n",
        "# Specifying the bin endpoints\n",
        "data[\"a_bucketized2\"] = pd.cut(data[\"a\"], bins=[-100,3.5, 9.5, 10])\n",
        "data"
      ],
      "metadata": {
        "id": "-HkJ0dK5yltO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# interaction variables\n",
        "data[\"ab\"] = data[\"a\"] * data[\"b\"]\n",
        "data"
      ],
      "metadata": {
        "id": "dtVNrI4m3UfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# notice that the bucketized columns are categorical\n",
        "data.dtypes"
      ],
      "metadata": {
        "id": "NJRzbgkHFp-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dealing with categorical variables"
      ],
      "metadata": {
        "id": "LRwPVcQJ2c4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame({\n",
        "    'boro': ['Brooklyn', 'Bronx', 'Bronx', 'Manhattan', 'Queens', 'Brooklyn'],\n",
        "    'salary' : [1000, 2000, 3000, 4000, 5000, 6000],\n",
        "    'satisfaction' : [4,3,3,5,1,2]})\n",
        "data.dtypes"
      ],
      "metadata": {
        "id": "EzjYkuPq2g8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, we want to cast the variable to type category\n",
        "data[\"boro\"] = data[\"boro\"].astype(\"category\")\n",
        "data[\"satisfaction\"] = data[\"satisfaction\"].astype(\"category\")\n",
        "data.dtypes"
      ],
      "metadata": {
        "id": "kGBUFK9x24vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Categories inferred from data.\n",
        "data[\"satisfaction\"].cat.categories"
      ],
      "metadata": {
        "id": "nnS0gUnf3-fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Notice that a boro named \"Staten Island\" is missing from the data.\n",
        "# In cases where we want to determine the categories in advance\n",
        "# (before we see the data), the right way to do this is as follows\n",
        "boro_type = pd.CategoricalDtype(categories=['Manhattan', 'Bronx', 'Brooklyn', \"Queens\", \"Staten Island\"])\n",
        "data[\"boro\"] = data[\"boro\"].astype(boro_type)\n",
        "data.dtypes"
      ],
      "metadata": {
        "id": "7XVpzZ9JGii3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ordinal encoding\n",
        "# Note that the encoding is based on the order that we determined in the\n",
        "# previous cell\n",
        "data[\"boro_ordinal\"] = data[\"boro\"].cat.codes\n",
        "data"
      ],
      "metadata": {
        "id": "7VtZZrP6HEHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding\n",
        "data_dummies = pd.get_dummies(data, columns=['boro', 'satisfaction'])\n",
        "data_dummies"
      ],
      "metadata": {
        "id": "o87-EwoxHXo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Impact (Stats) encoding\n",
        "stats = data['salary'].groupby(data['boro']).agg(['mean'])\n",
        "stats"
      ],
      "metadata": {
        "id": "EdM6EgXjJBSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapper = {s : stats.loc[s,'mean'] for s in stats.index }\n",
        "data[\"boro_impact\"] = data[\"boro\"].map(mapper)"
      ],
      "metadata": {
        "id": "p9viEzTKJdKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "15LrOzlJJ_Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection (Filter Method)"
      ],
      "metadata": {
        "id": "nQfvHmT0Zt5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create synthetic data with label 0,1,2,...99\n",
        "# x1 the same as the label plus some small noise\n",
        "# x2 is minus the label, plus some small noise\n",
        "# x3 is simply random\n",
        "data = pd.DataFrame({\n",
        "    'x1' : np.arange(100) + np.random.rand(100) * 10,\n",
        "    'x2' : -np.arange(100) + np.random.rand(100) * 20,\n",
        "    'x3' : np.random.rand(100),\n",
        "    'y' : np.arange(100)\n",
        "})\n",
        "data.head()"
      ],
      "metadata": {
        "id": "s6S3DVrbZ598"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's compute different types of correlation between all features, and y"
      ],
      "metadata": {
        "id": "hqnO0P_9aqgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pearson = data.corr(method='pearson')\n",
        "pearson"
      ],
      "metadata": {
        "id": "V0IcuicraxS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spearman = data.corr(method=\"spearman\")\n",
        "spearman"
      ],
      "metadata": {
        "id": "cdneWQfHa-vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can rank the features x1,x2,x3 in decreasing abs-value-of-corr-with-y\n",
        "# order.\n",
        "# (You can do the same thing using Pearson)\n",
        "sorted = spearman['y'].abs().sort_values(ascending=False)\n",
        "sorted"
      ],
      "metadata": {
        "id": "5gKaZwgGbPpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the first two.  Don't forget to skip the first in the sorted list, which\n",
        "# is y itself.\n",
        "chosen_features = sorted.iloc[1:3].index\n",
        "chosen_features"
      ],
      "metadata": {
        "id": "1GqvQpPjcETw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Imbalanced Data"
      ],
      "metadata": {
        "id": "2TdrICIKog6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame({\n",
        "    'x' : np.random.rand(1000),\n",
        "    'y' : (np.random.rand(1000) > 0.9) # 90% False, 10% True\n",
        "})\n",
        "data['y'].value_counts()"
      ],
      "metadata": {
        "id": "hqdf3ZS7KUvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First let's create arrays of false and true indices\n",
        "false_indices = data.index[data['y']==False]\n",
        "true_indices = data.index[data['y']==True]"
      ],
      "metadata": {
        "id": "O-Vf5Hl_orAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subsample the abundant class\n",
        "# In next line, replace=False means that we don't allow repeted samples\n",
        "subsampled_false_indices = np.random.choice(false_indices, size = 100,replace=False)\n",
        "subsampled_data = pd.concat([data.loc[subsampled_false_indices], data.loc[true_indices]])\n",
        "subsampled_data['y'].value_counts()"
      ],
      "metadata": {
        "id": "VqROnSCjtEgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Oversample the rare class\n",
        "# In the next line, replace has to be True because there is no way to\n",
        "# create more positive samples without repetition.\n",
        "oversampled_true_indices = np.random.choice(true_indices, size = 1000,replace=True)\n",
        "oversampled_data = pd.concat([data.loc[false_indices], data.loc[oversampled_true_indices]])\n",
        "oversampled_data['y'].value_counts()"
      ],
      "metadata": {
        "id": "I4UXF9-5piJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting to Train/Val/Test"
      ],
      "metadata": {
        "id": "ca7k4Oo7ue_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "data = pd.DataFrame({\n",
        "    'x1' : np.random.rand(1000),\n",
        "    'x2' : np.random.rand(1000),\n",
        "    'y' : (np.random.rand(1000) > 0.5)\n",
        "})\n",
        "train, test = train_test_split(data, test_size=0.1, random_state=111)"
      ],
      "metadata": {
        "id": "sgBdro7ltbEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "B8lrBIRf8WXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "GY8H4W_jDAy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.shape"
      ],
      "metadata": {
        "id": "cQYL_K8e8gs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's also get a validation set\n",
        "train, val = train_test_split(train, test_size=0.1, random_state=222)"
      ],
      "metadata": {
        "id": "VxATRGMd8qDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "CcHQIVzQ8vnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val.shape"
      ],
      "metadata": {
        "id": "pRc8GBa78wnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can also do this by shuffling and the slicing\n",
        "data_shuffled = data.sample(frac=1) # random shuffle in Pandas\n",
        "train = data_shuffled.iloc[:700]\n",
        "val = data_shuffled.iloc[700:850]\n",
        "test = data_shuffled.iloc[850:]"
      ],
      "metadata": {
        "id": "5pFP1mh08yqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "vSRfiuLG9Qn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val.shape"
      ],
      "metadata": {
        "id": "jLzOmYCB9Rjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.shape"
      ],
      "metadata": {
        "id": "V5xv1A8Q_lGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zvUj5VGT_l9U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}